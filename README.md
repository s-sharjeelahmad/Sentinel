# ğŸ›¡ï¸ Sentinel - Semantic AI Gateway

**A production-ready semantic caching layer that reduces redundant LLM API calls by up to 90%.**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://www.docker.com/)

---

## ğŸš€ **What is Sentinel?**

Sentinel is a **semantic AI gateway** that sits between your application and LLM providers (Groq, OpenAI, etc.). It uses **embedding-based semantic caching** to:

- âœ… **Reduce costs by 70-90%** - Cache semantically similar queries
- âœ… **Improve latency by 200x** - 5ms cache hits vs 1000ms API calls
- âœ… **Prevent redundant API calls** - Exact and semantic matching
- âœ… **Track usage metrics** - Monitor hit rates, costs, latency
- âœ… **Production optimized** - Model pre-cached, modern FastAPI lifespan, strict validation

---

## ğŸ¯ **Key Features**

| Feature              | Description                                                           |
| -------------------- | --------------------------------------------------------------------- |
| **Semantic Caching** | Uses sentence transformers (384D embeddings) to match similar queries |
| **Multi-Provider**   | Support for Groq (default), OpenAI, and extensible architecture       |
| **Production-Ready** | Docker, async I/O, error handling, logging, metrics                   |
| **Fast**             | Redis-backed cache with connection pooling, instant startup           |
| **Configurable**     | Similarity thresholds, TTL, model selection                           |

---

## ğŸ“Š **How It Works**

```
User Query â†’ Sentinel
              â†“
         [1] Compute embedding (BERT)
              â†“
         [2] Check exact cache match? â†’ âœ… Return (5ms)
              â†“ No
         [3] Check semantic match? â†’ âœ… Return (50ms)
              â†“ No
         [4] Call LLM API â†’ Cache result â†’ Return (1200ms)
```

**Example:**

- Query 1: "What is 2+2?" â†’ Cache MISS â†’ Groq API â†’ 1234ms
- Query 2: "What is 2+2?" â†’ Cache HIT (exact) â†’ 5ms âš¡
- Query 3: "What's two plus two?" â†’ Cache HIT (semantic) â†’ 45ms ğŸ¯

**Savings:** 2 out of 3 queries served from cache = **66% cost reduction!**

---

## ğŸ› ï¸ **Tech Stack**

- **FastAPI** 0.104 - Async web framework
- **Redis** 7.0 - Distributed cache
- **Transformers** - sentence-transformers/all-MiniLM-L6-v2 (384D embeddings)
- **aiohttp** - Async HTTP client for LLM providers
- **Pydantic** - Request/response validation
- **Docker** - Containerization

---

## ğŸ“¦ **Quick Start**

### **1. Clone the Repository**

```bash
git clone https://github.com/s-sharjeelahmad/Sentinel.git
cd Sentinel
```

### **2. Set Up Environment**

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### **3. Configure Environment Variables**

Create a `.env` file:

```env
GROQ_API_KEY=your_groq_api_key_here
REDIS_URL=redis://localhost:6379
```

### **4. Start Redis**

```bash
# Option A: Using Docker
docker run -d -p 6379:6379 redis:7-alpine

# Option B: Local installation
redis-server
```

### **5. Run Sentinel**

```bash
python main.py
```

Server starts at `http://localhost:8001` ğŸš€

---

## ğŸ³ **Docker Deployment**

### **Local Testing with Docker Compose**

```bash
# Start both Sentinel + Redis
docker-compose up -d

# Check logs
docker-compose logs -f

# Stop
docker-compose down
```

### **Production Deployment**

```bash
# Build image
docker build -t sentinel:latest .

# Run container
docker run -d \
  -p 8001:8001 \
  -e GROQ_API_KEY=your_key \
  -e REDIS_URL=redis://your-redis:6379 \
  sentinel:latest
```

---

## ğŸŒ **Deploy to Cloud (Free)**

### **Option 1: Fly.io (Recommended)**

```bash
# Install Fly CLI
curl -L https://fly.io/install.sh | sh

# Login
fly auth login

# Deploy
fly launch
fly secrets set GROQ_API_KEY=your_key
fly deploy
```

**Free tier:** 3 shared VMs, 3GB storage, managed Redis

### **Option 2: Render.com**

1. Connect GitHub repo
2. Select "Web Service"
3. Set build command: `docker build`
4. Add environment variable: `GROQ_API_KEY`
5. Deploy!

**Free tier:** 750 hours/month

---

## ğŸ”¥ **API Usage**

### **POST /v1/query - Send a query**

```bash
curl -X POST http://localhost:8001/v1/query \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "What is quantum computing?",
    "model": "llama-3.1-8b-instant",
    "temperature": 0.7,
    "max_tokens": 500,
    "similarity_threshold": 0.75
  }'
```

**Response:**

```json
{
  "response": "Quantum computing is...",
  "cache_hit": false,
  "similarity_score": null,
  "matched_prompt": null,
  "provider": "groq",
  "model": "llama-3.1-8b-instant",
  "tokens_used": 123,
  "latency_ms": 1234.5
}
```

### **GET /v1/metrics - View cache statistics**

```bash
curl http://localhost:8001/v1/metrics
```

**Response:**

```json
{
  "total_requests": 100,
  "cache_hits": 67,
  "cache_misses": 33,
  "hit_rate_percent": 67.0,
  "stored_items": 45,
  "uptime_seconds": 3600
}
```

### **GET /health - Health check**

```bash
curl http://localhost:8001/health
```

---

## ğŸ“ˆ **Performance Benchmarks**

| Scenario                  | Latency | Cost     | Tokens |
| ------------------------- | ------- | -------- | ------ |
| **Cache MISS (LLM call)** | 1200ms  | $0.00005 | 50     |
| **Cache HIT (exact)**     | 5ms     | $0       | 0      |
| **Cache HIT (semantic)**  | 45ms    | $0       | 0      |

**Cost Savings Example (1000 requests):**

- Without cache: 1000 Ã— $0.00005 = **$0.05**
- With 70% hit rate: 300 Ã— $0.00005 = **$0.015**
- **Savings: 70% ($0.035)**

---

## ğŸ“š **Documentation**

- [API Documentation](docs/api.md)
- [System Design](docs/system.md)
- [Decision Log](docs/decisions.md)
- [Learning Notes](docs/notes.md) - Comprehensive guide for understanding the system

---

## ğŸ›¡ï¸ **Production Considerations**

### **Security**

- âœ… Environment variables for secrets
- âœ… Input validation with Pydantic
- âš ï¸ Add rate limiting (nginx or Redis-based)
- âš ï¸ Add authentication (API keys/JWT)

### **Scalability**

- âœ… Async I/O for concurrent requests
- âœ… Redis connection pooling
- âš ï¸ Add horizontal scaling with load balancer
- âš ï¸ Add Redis clustering for high availability

### **Monitoring**

- âœ… Structured logging
- âœ… Health checks
- âš ï¸ Add Prometheus metrics
- âš ï¸ Add error tracking (Sentry)

---

## ğŸ¤ **Contributing**

Contributions welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ **License**

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ **Acknowledgments**

- [FastAPI](https://fastapi.tiangolo.com/) - Modern web framework
- [Groq](https://groq.com/) - Blazing-fast LLM inference
- [Sentence Transformers](https://www.sbert.net/) - Semantic similarity
- [Redis](https://redis.io/) - High-performance caching

---

## ğŸ“§ **Contact**

**Sharjeel Ahmad**

- GitHub: [@s-sharjeelahmad](https://github.com/s-sharjeelahmad)
- Project: [Sentinel](https://github.com/s-sharjeelahmad/Sentinel)

---

**Built with â¤ï¸ for learning and production use**
